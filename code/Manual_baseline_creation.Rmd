---
title: "Output evaluation"
author: "Fabienne Lind"
date: "2024-04-20"
output: html_document
---


We have managed to get an automated measurement for the variable climate activism. **But how valid is this measurement?** Does our small set of keyword represent the concept adequately?

A common procedure in automated content analysis is to test construct validity. We ask:
How close is this automated measurement to a more trusted measurement: Human understanding of text.
Let's put this to practice. 

## Dictionary validation with a human coded baseline

To validate the dictionary, we compare the classifications of the dictionary with the classifications of human coders. 

We create the human coded baseline together. 

Let's load the subset of the news articles that we classified with the search string. 

```{r setup, include=FALSE}

articles_hits <- read.csv("https://raw.githubusercontent.com/fabiennelind/Going-Cross-Lingual_Workshop/master/data/climate_news_d_annotated.csv")

```


### Intercoder reliability test

To ensure the quality of our manual coding, we first perform an intercoder reliability test. For this tutorial, we select a random set of 6 articles. In a real study the number of observations coded by several coders should of course be higher.  

```{r}

colnames(articles_hits)
articles_germany_de <- subset(articles_hits, Country == "Germany")
articles_uk_en <- subset(articles_hits, Country == "UK")


set.seed(57)# setting a seed ensures that the random selection can be repeated in the same way
library(dplyr)

intercoder_set_germany_de <- sample_n(articles_germany_de, 3) 
intercoder_set_uk_en <- sample_n(articles_uk_en, 3)
intercoder_set <- bind_rows(intercoder_set_germany_de, intercoder_set_uk_en)

```


We now add an empty column called `climate_activism_m`, so that coders can enter the manual codes. And delete the classification result.

```{r}

intercoder_set$climate_activism_m <- "" 
intercoder_set <- subset(intercoder_set, select =-c(climate_activism))

```

We then create several duplicates of the intercoder reliability set, one for each coder. We create separate files so that coders can so that everyone codes individually and does not peek by mistake. 
To each of these sets we add the coder name in a new column called `coder_name`.

We need two volunteers!:)

# Brainstorming: 

What characteristics would an ideal pool of coders have for this task? 

Can we meet these criteria with our group? 

If not, what are next best strategies?


```{r}

intercoder_set_coder1 <- intercoder_set
intercoder_set_coder1$coder_name <- "Coder1"

intercoder_set_coder2 <- intercoder_set
intercoder_set_coder2$coder_name <- "Coder2"

```

We then want to save the data sets in google sheets. Detailed instructions about the conncection of **R** and **Google Sheets** can be found in  [this](https://googlesheets4.tidyverse.org/articles/drive-and-sheets.html) and [this ](https://googlesheets4.tidyverse.org/articles/drive-and-sheets.html) tutorial.

The two packages needed here are **googledrive** and **googlesheets4**.

```{r}

#install.packages("googledrive")
#install.packages("googlesheets4")
library(googledrive)
library(googlesheets4)

```

```{r}

# Authentication
drive_auth(email ="fabienne.lind@gmail.com")
gs4_auth(token = drive_token())
drive_user()

```

We now save the datasets for the intercoder reliability test as Google Sheets with the function `gs4_create`. 

```{r}

sheet_id1<- gs4_create("intercoder_set_coder1",sheets = intercoder_set_coder1)
sheet_id2<- gs4_create("intercoder_set_coder2",sheets = intercoder_set_coder2)

```

Ready to code? Please open just the one that was created for you. Read the column `text`. If the text is about climate activism insert `1` in the column `climate_activism_m`. Enter a `0` in `climate_activism_m` if the text is not about climate activism.



After you finished coding, we read all sheets back into Rstudio (now with manual classifications).

```{r}

intercoder_set_coder1c <- read_sheet(sheet_id1)
intercoder_set_coder2c <- read_sheet(sheet_id2)

```

All dataframes are combined into one dataframe with the function `rbind`.

```{r}

reliability_set <- rbind(intercoder_set_coder1c, intercoder_set_coder2c) 

```

Too calculate the agreement between coders, we first restructure the `reliability_set` a bit (the different coders become variables). 'ID' is the name of our id variable. 'coder_name' is the column with the different coder ids. And 'climate_activism_m' is the variable for which we seek to test intercoder reliability.

```{r}
class(reliability_set)
reliability_set <- as.data.frame(reliability_set)
class(reliability_set$ID)
```



```{r}

#install.packages("reshape2")
library(reshape2) 

reliability_restructured <- dcast(reliability_set, ID ~ coder_name, value.var = "climate_activism_m", fun.aggregate = NULL)

reliability_transp <- t(reliability_restructured) # transpose data frames (rows to columns, columns to rows)
reliability_matrix <- data.matrix(reliability_transp) # convert df.t to matrix 
reliability_matrix_final <- reliability_matrix[-1,] # delete first row of matrix


```

The package **irr** allows to calculate various coefficients of intercoder reliability. 
We calculate Krippendorff's alpha for this example.

```{r}

#install.packages("irr")
library(irr)  

alpha_de <- kripp.alpha(reliability_matrix_final, method ="nominal") # select the appropriate method, nominal is default,
alpha_de

```

What was challenging?
What can we do to improve the score if necessary?

If alpha is large enough, we consider the quality of our manual coding as sufficient. We can then start with the creation of a larger manual baseline to be compared with the dictionary classifications.

## Creating a manually coded baseline

We pick an equal amount of texts per case and language randomly. For this example, we just pick 15 articles from Germany and 15 articles from the UK.

```{r}

#install.packages("dplyr")
library(dplyr)

set.seed(789)# setting a seed ensures that the random selection can be repeated in the same way
manual_set_germany_de <- sample_n(articles_germany_de, 15)
manual_set_uk_en <- sample_n(articles_uk_en, 15)
manual_set <- bind_rows(manual_set_germany_de, manual_set_uk_en)

```

We add again an empty column called `climate_activism_m`, for coders to enter the manual codes. This time, we also add an empty column for the coder names.

```{r}

manual_set$climate_activism_m <- "" 
manual_set$coder_name <- ""
manual_set <- subset(manual_set, select =-c(climate_activism))


```

We create a google sheet for the task with `gs4_create`. 

```{r}

sheet_id_manual <- gs4_create("manual_set", sheets = manual_set)

```


Please open the sheet in your browser. 

Enter a coding name (free to pick) in the column `coder_name` for a couple of rows first. Then start to enter 1 (climate activism mentioned) or 0 (not mentioned) in the column `climate_activism_m` for the rows with your coding name. Our goal is to finish coding of all articles.


After you finish coding, we read all sheets back into Rstudio (now with manual classifications).

```{r}

manual_set_coded <- read_sheet(sheet_id_manual)

```

We need to create a data set, where the manual and automated classifications are included.

```{r}

manual_set_coded <- subset(manual_set_coded, select = c("ID", "climate_activism_m"))# we need only 2 columns from the manual set
articles_coded_d_m <- left_join(manual_set_coded, articles_hits, by ="ID")
                           
```


## Compare automated with manual classifications 

We compare the automated classification (in column `climate_activism`) with the manual classifications (in column `climate_activism_m`) we use three metrics: Recall, Precision, and F1.
The metrics inform us about the quality of the dictionary. All three metrics range from 0 to 1. 
We assume that our manual classification identified all relevant articles (here: texts that are about climate activism).


To calculate the three metrics, we need first to create three new columns via some recoding. 

The column `Relevant_andRetrieved` includes a 1 if the manual coder and the dictionary coded 1. = True positive
The column `Relevant_notRetrieved` includes a 1 if the manual coder coded 1 but the dictionary coded 0. = False negative
The column `notRelevant_butRetrieved` includes a 1 if the manual coder coded 0 but the dictionary coded 1. = False positive

```{r}

articles_coded_d_m$Relevant_andRetrieved[articles_coded_d_m$climate_activism_m == 1 & articles_coded_d_m$climate_activism== 1 ] <- 1
articles_coded_d_m$Relevant_notRetrieved[articles_coded_d_m$climate_activism_m == 1 & articles_coded_d_m$climate_activism == 0 ] <- 1
articles_coded_d_m$notRelevant_butRetrieved[articles_coded_d_m$climate_activism_m == 0 & articles_coded_d_m$climate_activism == 1 ] <- 1

```

### Recall 

By inspecting recall we can say how many relevant articles are retrieved by the dictionary.
A recall of 1.0 means that our dictionary retrieved all relevant articles. 
A recall of 0.8 means that our dictionary retrieved 80% of all relevant articles. 

To obtain recall, we calculate:

```{r}

recall <- (sum(articles_coded_d_m$Relevant_andRetrieved, na.rm=TRUE))/(sum(articles_coded_d_m$Relevant_notRetrieved, na.rm=TRUE) + (sum(articles_coded_d_m$Relevant_andRetrieved, na.rm=TRUE)))
recall


```


### Precision 

By inspecting precision we can say how many retrieved articles are relevant.
A precision of 1,0 means that all articles retrieved by the dictionary are relevant. 
A precision of 0.8 means that 80% of the articles that our dictionary retrieved are relevant articles. 

To obtain precision, we calculate:

```{r}

precision <- (sum(articles_coded_d_m$Relevant_andRetrieved, na.rm=TRUE))/(sum(articles_coded_d_m$notRelevant_butRetrieved, na.rm=TRUE) + (sum(articles_coded_d_m$Relevant_andRetrieved, na.rm=TRUE)))
precision # 

```


### F1

F1 is the harmonic mean between recall and precision. 

To obtain F1, we calculate:

```{r}

F1 <- (2 * precision * recall)/(precision + recall)
F1

```

Questions: 

- Say we have a precision of .9 but a recall of .1, what does this mean for the quality of our dictionary?

- How are the results comparing cases and languages?

- What can we do to improve recall?

- What can we do to improve precision?



