---
title: "Implementing input alignment via machine translation"
author: "Fabienne Lind"
date: "October, 2022"
output:
  html_document:
    df_print: paged
---

# Implementing input alignment via machine translation

## Data

For the next tasks, we will work with an example text data, headlines from news articles about migration. The data set is a subset of the [REMINDER media corpus](https://doi.org/10.11587/IEGQ1B).

Let's load the data first and take a look. Each row represents one news article. How many articles per country do we have?

```{r}

articles <- read.csv("https://raw.githubusercontent.com/fabiennelind/Workshop_Multilingual-Text-Analysis_and_Comparative-Research/master/data/multilingual_data_annotated.csv")

#as.data.frame(articles)
table(articles$country) 

```



Let us now inspect the column `headline`, the column with our text to be pre-processed. 

```{r}

head(articles$headline) # show the first lines of the column
class(articles$headline) # check the class of the column.
articles$headline <- as.character(articles$headline) # Change the class to character.
class(articles$headline) # check if the edit worked.

```

For our exercise, we work with the headlines of the corpus parts published in Germany, Spain, and the UK separately. Thus, we first need to apply a filter. To select a subset of the data, we can use for example `subset`. We save the German subset in dataframe objects called `articles_de`, the Spanish part in a dataframe called `articles_es`, the English part in a dataframe called `articles_en`.

```{r}

articles_de <- subset(articles, country == "Germany")
articles_es <- subset(articles, country == "Spain")
articles_en <- subset(articles, country == "UK")

```



## 1. Translation prize

One approach to process the multilingual documents all together is to translate them first into a common language (= input alignment). Translation can also be useful or necessary to be performed on keywords of a search string, keywords of a dictionary, codebook instructions, extracted Part-of-Speech etc. 

Next to manual translation, machine translation can be performed. 

- [DeepL](https://www.rstudio.com)

To calculate the prize beforehand, I recommend to count the number of characters first for example with `n_char`.

```{r}

#number of characters DE sample
n_char <- nchar(articles_de$headline)# whitespaces are also counted 
n_char_de <- sum(n_char, na.rm = T) 

#number of characters ES sample
n_char <- nchar(articles_es$headline)# whitespaces are also counted 
n_char_es <- sum(n_char, na.rm = T) 



#Calculate Prize
translation_prize_case <- (((n_char_de + n_char_es)/1000000)*20) #Neutral Translation Model Online Predictions (20 Dollar/1M Characters)
translation_prize_case 

```


## 2. Translation with the DeepL API

DeepL offers the [DeepL API Free](https://www.deepl.com/en/docs-api/) which allows a maximum of 500,000 characters/month to be translated for free. To use the API, it is necessary to create an account and provide your credit card details. After creating an account you will receive an Authentication Key (You find it your DeepL account settings). With [deeplr](https://github.com/zumbov2/deeplr) [(Zumbach & Bauer, 2021)](https://CRAN.R-project.org/package=deeplr), a new wrapper for the DeepL API it is easily possible to work with it from R. 


```{r}
#install.packages("deeplr")
library(deeplr)

#show available languages
#langs <- available_languages2("my_key") #Replace `my_key` with your Authentication Key.
#as.data.frame(langs)

#monitor your usage
#usage2("my_key") #Replace `my_key` with your Authentication Key.


```


This code will translate the text of column `headline_de` and `headline_es` and save the result in `headline_mt`. The source languages is guessed automatically if `source_lang = NULL`. The target language is here defined as `EN` = English.


```{r}

#translate DE to EN
#articles_de$headline_mt <- translate2(
#  articles_de$headline,
#  source_lang = DE, #if source_lang = NULL, the source language will be guessed
#  target_lang = "EN",
#  auth_key = "my_key" #Replace `my_key` with your Authentication Key.
#  )

as.data.frame(articles_de)

#translate ES to EN
#articles_es$headline_mt <- translate2(
#  articles_es$headline,
#  source_lang = ES, #if source_lang = NULL, the source language will be guessed
#  target_lang = "EN",
#  auth_key = "my_key" #Replace `my_key` with your Authentication Key.
#  )

# No need to translate the Uk subset of course

```

Just in case, with the following command you can just read in the translated headlines for you to move on to the next task (in case the translation did not work).

```{r}

#articles <- read.csv("https://raw.githubusercontent.com/fabiennelind/Workshop_Multilingual-Text-Analysis_and_Comparative-Research/master/data/multilingua#l_data_annotated_translated.csv")

```


## 3: UDPipe for Lemmatization

We will now use the R package [UDIpipe](https://cran.r-project.org/web/packages/udpipe/index.html) to perform lemmatization. 

For additional UDPipe instructions, I can recommend [this](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html) tutorial by the author Jan Wijffels.

We want to lemmatize the headlines. The R package does this based on language models. We download the model first and use then `udpipe_annotate` to perform  lemmatization. The function does also tokenisation, tagging, and dependency parsing.

We do this for all 1500 English headlines.


```{r}

library(udpipe)
articles$headline_mt <- as.character(articles$headline_mt)
udmodel_en <- udpipe_download_model(language = "english")
udmodel_en <- udpipe_load_model(file = udmodel_en$file_model)
udi_en <- udpipe_annotate(udmodel_en, x = articles$headline_mt, doc_id = articles$id)
udi_en <- as.data.frame(udi_en)

```

Now, it is time for some data reorganization. 
A code snippet that allows to add a new column with the lemmatized text to your corpus. 

```{r}

#udi_es_pos <- subset(udi_es, upos %in% c("NOUN")) # in case, you like to filter only specific lemmas you can activate this line, here one option to use only nouns 

library(dplyr)
udi_en_lemma <- udi_en %>% 
  group_by(doc_id) %>% 
  mutate(headline_lemma = paste0(lemma, collapse = " "))

names(udi_en_lemma)[names(udi_en_lemma)=="doc_id"] <- "id" #rename column doc_id and use the article id of your corpus 'id'
udi_en_lemma <- subset(udi_en_lemma, select = c(id, headline_lemma))


udi_en_lemma$dupl <- duplicated(udi_en_lemma$id) #tag duplicated rows
udi_en_lemma <- subset(udi_en_lemma, dupl == FALSE)# select only unique rows
udi_en_lemma <- subset(udi_en_lemma, select = c(id, headline_lemma)) #select only relevant columns
#udi_de_lemma$id <- as.character(udi_de_lemma$id) 

#add the new column 'headline_lemma' to the corpus

articles$id <- as.character(articles$id)
articles <- left_join(articles, udi_en_lemma, by = "id")

```



## Order of pre-processing steps


- If you plan to work with machine translated text, translate the original text. Lemmatization/POS Tagging is then performed afterwards with the translated text.

- If you plan to work with original language text (seperate analysis), then lemmatize language by language with the respective language model.

## 3: spacyr for Named Entity Extraction

Set up  and select a language model

Visit https://spacy.io/usage/models and select a language model. In this example, we use the model for English.

```{r pressure, echo=FALSE}

library(spacyr)
# spacy_finalize() #close previous session (where previous language model was loaded)
spacy_initialize(model = "en")

```

# Extract Entities

We will now apply the function `spacy_extract_entity`, extracts entities from text (here stored in the dataframe column `text_mt` without first parsing the entire text:


```{r}
corpus_entities <- spacy_extract_entity(corpus$text_mt) # specify the text column

```


# Specify the enty_types wanted 

Let's see what types of entities were extracted.

```{r}
table(corpus_entities$ent_type)
```


In some szenarios we might be interested in specifc entity types only.

For an overview of available entity types see https://spacy.io/usage/linguistic-features#named-entities

# 'PER'  = Named person or family; 
# 'GPO' = Geopolitical entity, i.e. countries, cities, states..
# 'ORG' =  Named corporate, governmental, or other organizational entity.
# 'MISC' =  #Miscellaneous entities, e.g. events, nationalities, products or works of art. 

For this example, we decide to investigate only 'LOC', 'GPE' 'PER'. The following code is an example of how these types are selected from the output generated previously and aggregated back on document level as string column 

```{r}

corpus_entities_sub <- subset(corpus_entities, ent_type == 'PER' | ent_type == 'GPE' | ent_type == 'LOC')

# Aggregate all extracted entities and the corresponding entity_type per article & list them in one string separated by ; 

library(dplyr)
#entity text
entity_agg <- corpus_entities_sub %>% 
  group_by(doc_id) %>% 
  mutate(entity = paste0(text, collapse = "; "))

entity_agg <- subset(entity_agg, select = c(doc_id, entity))

entity_agg$dupl <- duplicated(entity_agg$doc_id) #tag duplicated rows
entity_agg <- subset(entity_agg, dupl == FALSE)# select only unique rows
entity_agg <- subset(entity_agg, select = c(doc_id, entity))

